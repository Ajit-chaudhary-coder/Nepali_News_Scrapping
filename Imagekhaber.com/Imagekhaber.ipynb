{"cells":[{"cell_type":"markdown","metadata":{"id":"AuXBvuwijXFu"},"source":["###** Education**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AU7IWcSTYb_M"},"outputs":[],"source":["import requests\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","from urllib.parse import quote\n","from time import sleep\n","from random import randint\n","\n","# Initialize lists to store the data\n","URL = []\n","Title = []\n","Category = []\n","Author_name = []\n","Author_url = []\n","Date = []\n","Content = []\n","Description = []\n","\n","base_url = 'https://www.imagekhabar.com/'\n","\n","# Function to get all the content of an article\n","def get_article_detail(article_url):\n","    response = requests.get(article_url)\n","    if response.status_code == 200:\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","\n","        # Extract author detail\n","        author_box = soup.find('a', class_='uk-link-reset uk-margin-left uk-margin-right')\n","        if author_box:\n","            author_name = author_box.get_text(strip=True)\n","            author_url = author_box['href']\n","        else:\n","            author_name = None\n","            author_url = None\n","\n","        # Extract date and time\n","        date_time_box = soup.find('div', class_='post-time uk-visible@s')\n","        if date_time_box:\n","            date_time = date_time_box.get_text(strip=True)\n","        else:\n","            date_time = None\n","\n","        # Find and extract the content in <p> tags within the <article> section\n","        article_section = soup.find('article', class_='post-entry uk-text-center')\n","        if article_section:\n","            paragraphs = article_section.find_all('p')\n","            content = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n","        else:\n","            content = None\n","\n","        # Define description (if needed, adjust extraction logic here)\n","        description = None  # Update this based on your specific needs\n","\n","        # Append data to lists\n","        Description.append(description)\n","        Author_name.append(author_name)\n","        Author_url.append(base_url + author_url if author_url else None)\n","        Date.append(date_time)\n","        Content.append(content)\n","\n","# Function to scrape a specific section and a range of pages\n","def scrape_section(section, start_page=1, end_page=2):\n","    # Encode the section part of the URL\n","    encoded_section = quote(section)\n","    category = section\n","\n","    sleep(randint(2, 10))\n","    for page in range(start_page, end_page + 1):\n","        url = f'https://www.imagekhabar.com/news/category/{encoded_section}/page/{page}/'\n","        print(f\"Scraping URL: {url}\")\n","        response = requests.get(url)\n","\n","        # Check if the request was successful\n","        if response.status_code == 200:\n","            # Parse the HTML content\n","            soup = BeautifulSoup(response.content, 'html.parser')\n","\n","            # Find all article items\n","            article_items = soup.find_all('div', class_='uk-card-body')\n","            if not article_items:\n","                print(\"No articles found.\")\n","            for article_item in article_items:\n","                # Extract the title and title link\n","                title_box = article_item.find('h3')\n","                if title_box:\n","                    title = title_box.get_text(strip=True)\n","                    title_link = title_box.find('a')['href']\n","\n","                    # Append data to lists\n","                    URL.append(title_link)\n","                    Title.append(title)\n","                    Category.append(category)\n","\n","                    # Scrape article details\n","                    get_article_detail(title_link)\n","\n","                    # Sleep to avoid overwhelming the server\n","                    sleep(randint(2, 10))\n","\n","# Example usage\n","scrape_section('शिक्षा', 1, 2)\n","\n","# Ensure all lists are the same length\n","max_len = max(len(URL), len(Title), len(Category), len(Description), len(Content), len(Date), len(Author_url), len(Author_name))\n","\n","# Fill shorter lists with None to match the length\n","URL.extend([None] * (max_len - len(URL)))\n","Title.extend([None] * (max_len - len(Title)))\n","Category.extend([None] * (max_len - len(Category)))\n","Description.extend([None] * (max_len - len(Description)))\n","Content.extend([None] * (max_len - len(Content)))\n","Date.extend([None] * (max_len - len(Date)))\n","Author_url.extend([None] * (max_len - len(Author_url)))\n","Author_name.extend([None] * (max_len - len(Author_name)))\n","\n","# Create a DataFrame\n","data = {\n","    'URL': URL,\n","    'Title': Title,\n","    'Description': Description,\n","    'Content': Content,\n","    'Category': Category,\n","    'Date': Date,\n","    'Author_url': Author_url,\n","    'Author_name': Author_name\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Save to CSV file\n","df.to_csv('imagekhaber_data_education.csv', index=False)\n","print(\"Data saved to scraped_data.csv\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMUVkNXr14icO+tmCeH91b+","collapsed_sections":["AuXBvuwijXFu"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
