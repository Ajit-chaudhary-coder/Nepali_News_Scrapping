{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Np4VhBppfXFy"},"outputs":[],"source":["import pandas as pd\n","from bs4 import BeautifulSoup as BS\n","import urllib3\n","from time import sleep # Import the sleep function\n","from random import randint # Import the randint function\n","from datetime import datetime, timedelta\n","\n","# Mount Google Drive (if not already mounted)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# List of categories to scrape\n","categories = [\"business\", \"opinion\", \"sports\", \"national\", \"entertainment\", \"feature\", \"world\",\"blog\",\"koseli\",\"diaspora\",\"Education\"]  # Add more categories as needed\n","\n","# Get today's date and calculate dates for the past 2 days\n","today = datetime.now()\n","dates = [today - timedelta(days=i) for i in range(2)]  # Adjust the number of days as needed\n","\n","# Initialize an empty list to store all the news data\n","all_news_data = []\n","\n","# Loop through each category\n","sleep(randint(2,10))\n","for category in categories:\n","    # Loop through each date\n","    for date in dates:\n","        # Format the date as YYYY/MM/DD\n","        formatted_date = date.strftime(\"%Y/%m/%d\")  # Adjust the date format if needed\n","\n","        # Construct the URL for the news page\n","        sleep(randint(2,10))\n","        url = f\"https://ekantipur.com/{category}/{formatted_date}\"\n","\n","        # Initialize HTTP connection pool\n","        http = urllib3.PoolManager()\n","        http.addheaders = [('User-agent', 'Mozilla/61.0')]\n","\n","        # Fetch the web page content\n","        sleep(randint(2,10))\n","        web_page = http.request('GET', url)\n","        soup = BS(web_page.data, 'html5lib')\n","\n","        # Loop through all the divs with '.normal` class found in the webpage\n","        for row in soup.select(\".normal\"):\n","            # title is of h2 element\n","            title = row.find(\"h2\")\n","\n","            # extract the href attribute of a of a title i.e. URL of the link\n","            title_link = url.split(f\"/{category}\")[0] + title.a.get(\"href\")\n","\n","            # description is on p element\n","            description = row.find(\"p\").text\n","\n","            # get title text\n","            title_text = title.text\n","\n","            # Fetch the news page content\n","            sleep(randint(2,10))\n","            news_page = http.request('GET', title_link)\n","            news_soup = BS(news_page.data, 'html5lib')\n","\n","            # find the date and time\n","            date_element = news_soup.select_one(\"div.time-card\") # Select the span with class \"normal\"\n","            if date_element:\n","                date = date_element.text.strip()  # Extract text and remove extra spaces\n","            else:\n","                date = None\n","\n","            # Check if author element exists before accessing its attributes\n","            sleep(randint(2,10))\n","            author_element = news_soup.select_one(\".author\")\n","            if author_element:\n","                # find the author URL and author name\n","                author_url = author_element.a.get(\"href\")\n","                author_name = author_element.text\n","            else:\n","                author_url = None\n","                author_name = None\n","\n","            # find the news content\n","            news_content = \"\"\n","            for content in news_soup.select_one(\"div.description\").findAll(\"p\"):\n","                news_content += content.text.strip() + \" \"\n","\n","            content = news_content.strip()\n","\n","            # Store the extracted data in a dictionary\n","            news_item = {\n","                \"Title\": title_text,\n","                \"URL\": title_link,\n","                \"Date\": date,\n","                \"Author\": author_name,\n","                \"Category\": category,\n","                \"Author URL\": author_url,\n","                \"Description\": description,\n","                \"Content\": content\n","            }\n","\n","            # Append the news item to the list\n","            all_news_data.append(news_item)\n","\n","# Create a Pandas DataFrame from the collected data\n","df = pd.DataFrame(all_news_data)\n","\n","\n","\n","\n","# Save the DataFrame to a CSV file in your Google Drive\n","df.to_csv('/content/drive/My Drive/kantipurdataset.csv', index=False) # Change 'My Drive' to your Drive folder name if needed\n","print(\"CSV file saved to Google Drive!\")\n","\n","\n"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1tAUz6bZ6dHrN4rVbdQmfURt7S1hAGql5","authorship_tag":"ABX9TyNBDlsw/W5u3ByN7oGgKEMe"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}