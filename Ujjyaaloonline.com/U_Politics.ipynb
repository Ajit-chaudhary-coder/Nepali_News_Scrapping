{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO9Szuq7yj8WxGM+esWphTG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ozsl9HfxtF44"},"outputs":[],"source":["\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from bs4 import BeautifulSoup\n","import re\n","import csv\n","from time import sleep\n","from random import randint\n","\n","# Define headers to make the request look like it's coming from a web browser\n","headers = {\n","    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n","}\n","import requests\n","# Function to extract data from a single page\n","def extract_data_from_page(url):\n","    response = requests.get(url, headers=headers)\n","    if response.status_code != 200:\n","        print(f\"Failed to retrieve the page: {response.status_code}\")\n","        return []\n","\n","    html_content = response.content\n","\n","    # Parse the HTML content with BeautifulSoup\n","    soup = BeautifulSoup(html_content, 'html.parser')\n","\n","    # Extract the category from the base URL\n","    category = re.search(r'/category/([^?]+)', url).group(1)\n","\n","    # Find all article containers\n","    article_containers = soup.find_all('div', class_='col-md-4 padding-10')\n","    if not article_containers:\n","        print(\"No article containers found\")\n","        return []\n","\n","    # Initialize a list to store the extracted information\n","    articles = []\n","\n","    # Loop through each article container and extract the required information\n","    for idx, container in enumerate(article_containers):\n","        article_info = {}\n","\n","        # Extract the title and title URL\n","        title_tag = container.find('h3', class_='mb-15 fw-6 fz-20').find('a')\n","        if title_tag:\n","            title = title_tag.text.strip()\n","            title_url = \"https://ujyaaloonline.com\" + title_tag['href']\n","        else:\n","            print(f\"Title not found in container {idx}\")\n","            continue\n","\n","        # Extract the description\n","        description_tag = container.find('div', class_='grid-post')\n","        if description_tag and description_tag.find('img'):\n","            description = description_tag.find('img')['alt'].strip()\n","        else:\n","            print(f\"Description not found in container {idx}\")\n","            description = \"\"\n","\n","        # Extract the author URL and author name\n","        author_tag = container.find('a', href=re.compile(r'/author/'))\n","        if author_tag:\n","            author_url = \"https://ujyaaloonline.com\" + author_tag['href']\n","            author_name = author_tag.text.strip()\n","        else:\n","            print(f\"Author information not found in container {idx}\")\n","            author_url = \"\"\n","            author_name = \"\"\n","\n","        # Extract the date\n","        date_tag = container.find('span', class_='date')\n","        if date_tag:\n","            date = date_tag.text.strip()\n","        else:\n","            print(f\"Date not found in container {idx}\")\n","            date = \"\"\n","\n","        # Send a request to the title URL and get the content\n","        title_response = requests.get(title_url, headers=headers)\n","        if title_response.status_code != 200:\n","            print(f\"Failed to retrieve the title page: {title_url}\")\n","            content = \"\"\n","        else:\n","            title_html_content = title_response.content\n","            title_soup = BeautifulSoup(title_html_content, 'html.parser')\n","\n","            # Extract all the content from the <p style=\"text-align: justify;\"> elements\n","            content_paragraphs = title_soup.find_all('p', style=\"text-align: justify;\")\n","            content = \"\\n\".join([p.text.strip() for p in content_paragraphs])\n","\n","        # Store the extracted information in the dictionary\n","        article_info['Title'] = title\n","        article_info['URL'] = title_url\n","        article_info['Description'] = description\n","        article_info['Author_url'] = author_url\n","        article_info['Author_name'] = author_name\n","        article_info['Date'] = date\n","        article_info['Category'] = category\n","        article_info['Content'] = content\n","\n","        # Append the dictionary to the articles list\n","        articles.append(article_info)\n","\n","    return articles\n","\n","# Generate a list of page URLs to scrape\n","sleep(randint(2,10))\n","base_url = \"https://ujyaaloonline.com/category/politics?page=\"\n","start_page = 0\n","end_page = 10\n","\n","page_urls = [base_url + str(page) for page in range(start_page, end_page + 1)]\n","\n","# Initialize a list to store all articles from all pages\n","all_articles = []\n","\n","# Loop through each page URL and extract the data\n","for page_url in page_urls:\n","    articles = extract_data_from_page(page_url)\n","    all_articles.extend(articles)\n","\n","# Define the CSV file name\n","csv_file = 'extracted_articles.csv'\n","\n","# Define the CSV headers\n","csv_headers = ['URL','Title',  'Description', 'Content','Category', 'Date','Author_url', 'Author_name'  ]\n","\n","# Write the extracted information to the CSV file\n","with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n","    writer = csv.DictWriter(file, fieldnames=csv_headers)\n","    writer.writeheader()\n","    writer.writerows(all_articles)\n","\n","print(f\"Data successfully saved to {csv_file}\")\n"]}]}